{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_sparkdotnet",
      "display_name": "csharp"
    },
    "language_info": {
      "name": "csharp"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Referencing libraries\r\n",
        "Microsoft.SqlServer.DacFx has to be referenced using configurations cell which will cause the spark session to restart because of using the -f parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\r\n",
        "{ \"conf\": {\"spark.dotnet.extraPackages\": \"nuget:Microsoft.SqlServer.DacFx,160.6161.0\" }}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#r \"nuget:Newtonsoft.Json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#r \"nuget:SynapseQueryParserKernel\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load QueryStoreQueryText into dataframe\r\n",
        "The QueryStoreQueryText table is a Synapse SQL table that has the incremental loading of the QueryStore tables with this query \r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "SELECT txt.query_sql_text,txt.statement_sql_handle,qry.query_id,qry.object_id,qry.is_internal_query,qry.last_execution_time,SUM(count_executions) AS count_executions\r\n",
        "FROM sys.query_store_query_text txt\r\n",
        "INNER JOIN sys.query_store_query qry\r\n",
        "    ON Qry.query_text_id = txt.query_text_id\r\n",
        "JOIN sys.query_store_plan pln\r\n",
        "    ON qry.query_id=pln.query_id\r\n",
        "JOIN sys.query_store_runtime_stats runstate\r\n",
        "ON pln.plan_id=runstate.plan_id\r\n",
        "GROUP BY txt.query_sql_text,txt.statement_sql_handle,qry.query_id,qry.object_id,qry.is_internal_query,qry.last_execution_time    \r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val df_scala=spark.read.synapsesql(\"retaildw.dbo.QueryStoreQueryText\")\r\n",
        "df_scala.createOrReplaceTempView(\"vquerystorequerytext\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Back to spark .net and datatypes adjustment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "var df=spark.Sql(\"select * from vquerystorequerytext\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.PrintSchema();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "using Microsoft.Spark.Sql;\r\n",
        "df=(\r\n",
        "    df\r\n",
        "    .WithColumn(\"Command\",df[\"query_sql_text\"])\r\n",
        "    .WithColumn(\"last_execution_time\",df[\"last_execution_time\"].Cast(\"date\"))\r\n",
        "    .Drop(df[\"query_sql_text\"])\r\n",
        "    .WithColumn(\"sqlAnalytics\",Lit(\"\"))\r\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.PrintSchema();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Parse\r\n",
        "This is the most important step, creating a Spark UDF to be used to call the parsing library. the main class is *SynapseVisitor* and has only one public method which is *ProcessVisitor* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "using Newtonsoft.Json;\r\n",
        "using QueryParserKernel;\r\n",
        "using Microsoft.Spark.Sql;\r\n",
        "\r\n",
        "Func<Column,Column> parseSQL = Udf<string,string>(\r\n",
        "    strSQL => {\r\n",
        "        SynapseQueryModel model = new SynapseQueryModel();\r\n",
        "        SynapseVisitor synapseVisitor = new SynapseVisitor();\r\n",
        "        model = synapseVisitor.ProcessVisitor(strSQL);\r\n",
        "        var responseMessage = JsonConvert.SerializeObject(model);\r\n",
        "        return responseMessage;\r\n",
        "    }\r\n",
        ");\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df=df.WithColumn(\"sqlAnalytics\",parseSQL(df[\"Command\"]));\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Extract the schema and load the json into columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.PrintSchema();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "IEnumerable<Row> rows = df.Collect();\r\n",
        "var json_data = rows.First().GetAs<string>(\"sqlAnalytics\");\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "// Save and load to get the scahme \r\n",
        "\r\n",
        "var workspaceName=Env.GetWorkspaceName();\r\n",
        "FS.Mkdirs($\"/synapse/workspaces/{workspaceName}/sparkpools/tmp/\");\r\n",
        "FS.Put($\"/synapse/workspaces/{workspaceName}/sparkpools/tmp/json_data.json\",json_data,true);\r\n",
        "\r\n",
        "var df_schema=spark.Read().Json($\"/synapse/workspaces/{workspaceName}/sparkpools/tmp/json_data.json\");\r\n",
        "var json_schema=df_schema.Schema().Json;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Extract Columns from JSON column sqlAnalytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df=df.WithColumn(\"sqlAnalytics\", FromJson(Col(\"sqlAnalytics\"), json_schema)).Select(\"*\",\"sqlAnalytics.*\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "Display(df);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.PrintSchema();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "// Replace empty arrays with null\r\n",
        "using Microsoft.Spark.Sql;\r\n",
        "df = (\r\n",
        "    df\r\n",
        "    .WithColumn(\"JoinedTables\", When(Size(df[\"JoinedTables\"])==0, Lit(null)).Otherwise(df[\"JoinedTables\"]))\r\n",
        "    .WithColumn(\"JoinedColumns\",When(Size(df[\"JoinedColumns\"])==0, Lit(null)).Otherwise(df[\"JoinedColumns\"]))\r\n",
        "    .WithColumn(\"InsertStatementTargets\",When(Size(df[\"InsertStatementTargets\"])==0,Lit(null)).Otherwise(df[\"InsertStatementTargets\"]))\r\n",
        "    .WithColumn(\"DeleteStatementTargets\",When(Size(df[\"DeleteStatementTargets\"])==0,Lit(null)).Otherwise(df[\"DeleteStatementTargets\"]))\r\n",
        "    .WithColumn(\"Errors\", When(Size(df[\"Errors\"])==0,Lit(null)).Otherwise(df[\"Errors\"]))\r\n",
        "    .WithColumn(\"CopyStatementFrom\", When(Size(df[\"CopyStatementFrom\"])==0,Lit(null)).Otherwise(df[\"CopyStatementFrom\"]))\r\n",
        "    .WithColumn(\"CopyStatementInto\", When(Size(df[\"CopyStatementInto\"])==0, Lit(null)).Otherwise(df[\"CopyStatementInto\"]))\r\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "// With no errors\r\n",
        "df.Filter(\"errors is NULL\").Write().Mode(\"overwrite\").SaveAsTable(\"SynapseSqlAnalytics\");\r\n",
        "// with errors\r\n",
        "df.Filter(\"errors is NOT NULL\").Write().Mode(\"overwrite\").SaveAsTable(\"SynapseSqlAnalytics_Errors\");"
      ]
    }
  ]
}