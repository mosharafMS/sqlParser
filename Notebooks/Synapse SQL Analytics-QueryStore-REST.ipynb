{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Query history from Query Store\r\n",
        "The QueryStoreQueryText table is a Synapse SQL table that has the incremental loading of the QueryStore tables with this query \r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "SELECT txt.query_sql_text,txt.statement_sql_handle,qry.query_id,qry.object_id,qry.is_internal_query,qry.last_execution_time,SUM(count_executions) AS count_executions\r\n",
        "FROM sys.query_store_query_text txt\r\n",
        "INNER JOIN sys.query_store_query qry\r\n",
        "    ON Qry.query_text_id = txt.query_text_id\r\n",
        "JOIN sys.query_store_plan pln\r\n",
        "    ON qry.query_id=pln.query_id\r\n",
        "JOIN sys.query_store_runtime_stats runstate\r\n",
        "ON pln.plan_id=runstate.plan_id\r\n",
        "GROUP BY txt.query_sql_text,txt.statement_sql_handle,qry.query_id,qry.object_id,qry.is_internal_query,qry.last_execution_time   "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SynapseML configurations"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "  \"name\": \"synapseml\",\r\n",
        "  \"conf\": {\r\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.4\",\r\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\r\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12\",\r\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\r\n",
        "  }\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Key Parameter"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key=\"put the Azure Function key here\""
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load QueryStoreQueryText into dataframe"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\r\n",
        "val df_scala=spark.read.synapsesql(\"retaildw.dbo.QueryStoreQueryText\")\r\n",
        "df_scala.createOrReplaceTempView(\"vquerystorequerytext\")"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to pyspark and datatypes adjustment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.sql(\"select * from vquerystorequerytext\")"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=(\r\n",
        "    df\r\n",
        "    .withColumn(\"Command\",df[\"query_sql_text\"])\r\n",
        "    .withColumn(\"last_execution_time\",df[\"last_execution_time\"].cast(\"date\"))\r\n",
        "    .drop(df[\"query_sql_text\"])\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SynapseML HTTPTransformer\r\n",
        "Using SynapseML to call the REST API per record to parse the sql in the column Command"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from synapse.ml.io.http import http_udf,HTTPTransformer\r\n",
        "from pyspark.sql.functions import udf, col\r\n",
        "import requests\r\n",
        "\r\n",
        "\r\n",
        "# function to call rest API with code authentication\r\n",
        "def call_rest_api(sqlCommand):\r\n",
        "    url=\"https://synapsequeryparserfunc.azurewebsites.net/api/parse\"\r\n",
        "    # set headers\r\n",
        "    headers={'Content-Type': 'application/json', 'x-functions-key': key}\r\n",
        "    return requests.Request(method=\"Post\", url=url, data=sqlCommand, headers=headers)\r\n",
        "\r\n",
        "\r\n",
        "df=df.withColumn(\"request\",http_udf(call_rest_api)(col(\"Command\")))\r\n",
        "\r\n",
        "def get_response_body(resp):\r\n",
        "    return resp.entity.content.decode()\r\n",
        "\r\n",
        "client = (HTTPTransformer()\r\n",
        "          .setConcurrency(24)\r\n",
        "          .setInputCol(\"request\")\r\n",
        "          .setOutputCol(\"response\"))\r\n",
        "\r\n",
        "\r\n",
        "def get_response_body(resp):\r\n",
        "    return resp.entity.content.decode()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df= client.transform(df).select(\"*\", udf(get_response_body)(col(\"response\")).alias(\"sqlAnalytics\"))"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(df[\"request\"])\r\n",
        "df=df.drop(df[\"response\"])"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_json,schema_of_json,col\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "# read the schema of the json\r\n",
        "json_schema = spark.read.json(df.rdd.map(lambda row: row.sqlAnalytics)).schema\r\n",
        "\r\n",
        "#create columns out of the schema\r\n",
        "df=df.withColumn('sqlAnalytics', from_json(col('sqlAnalytics'), json_schema)).select(\"*\",\"sqlAnalytics.*\")"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace empty arrays with null\r\n",
        "from pyspark.sql.functions import regexp_replace,when,size\r\n",
        "df = (\r\n",
        "    df\r\n",
        "    .withColumn('joinedTables', when(size(df.joinedTables)==0, None).otherwise(df.joinedTables))\r\n",
        "    .withColumn('joinedColumns',when(size(df.joinedColumns)==0, None).otherwise(df.joinedColumns))\r\n",
        "    .withColumn('insertStatementTargets',when(size(df.insertStatementTargets)==0, None).otherwise(df.insertStatementTargets))\r\n",
        "    .withColumn('deleteStatementTargets',when(size(df.deleteStatementTargets)==0, None).otherwise(df.deleteStatementTargets))\r\n",
        "    .withColumn('errors',when(size(df.errors)==0, None).otherwise(df.errors))\r\n",
        "    .withColumn('copyStatementFrom',when(size(df.copyStatementFrom)==0, None).otherwise(df.copyStatementFrom))\r\n",
        "    .withColumn('copyStatementInto',when(size(df.copyStatementInto)==0, None).otherwise(df.copyStatementInto))\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write the dataframes"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With no errors\r\n",
        "df.filter('errors is NULL').write.mode(\"overwrite\").saveAsTable(\"SynapseSqlAnalytics\")\r\n",
        "# with errors\r\n",
        "df.filter('errors is NOT NULL').write.mode(\"overwrite\").saveAsTable(\"SynapseSqlAnalytics_Errors\")"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}